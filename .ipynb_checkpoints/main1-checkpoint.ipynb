{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a3cd77cb-31c3-4072-8a83-1f5de04401ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import d2l.torch as d2l\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "torch.manual_seed(0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a4eb567e-42e9-4006-a30f-266c2b8dae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.rcParams[\"figure.figsize\"] = (45,35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "135f6143-1f52-4f96-8455-a3f44c791aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_excel(f\"./data/flight_data_batch{i}.xlsx\")\n",
    "    df.insert(len(df.columns.tolist()), \"is_fail\", 0)\n",
    "    fail_df = pd.read_csv(f\"./data/Failures/Failure_Events_in_Batch_{i}.csv\")\n",
    "    \n",
    "    fails = set(zip(fail_df[\"flight_id\"], fail_df[\"failure_time\"]))\n",
    "    df.loc[df.apply(lambda row: (row[\"flight_id\"], row[\"time\"]) in fails, axis=1), \"is_fail\"] = 1\n",
    "\n",
    "    dfs = pd.concat([dfs, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04d73bdf-9560-45d2-8e9c-6d1cf23ab20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    df = pd.read_excel(f\"./data/flight_data_batch6_part{i}.xlsx\")\n",
    "    \n",
    "    df.insert(len(df.columns.tolist()), \"is_fail\", 0)\n",
    "    fail_df = pd.read_csv(f\"./data/Failures/Failure_Events_in_Batch_6_Part_{i}.csv\")\n",
    "    \n",
    "    fails = set(zip(fail_df[\"flight_id\"], fail_df[\"failure_time\"]))\n",
    "    df.loc[df.apply(lambda row: (row[\"flight_id\"], row[\"time\"]) in fails, axis=1), \"is_fail\"] = 1\n",
    "    \n",
    "    dfs = pd.concat([dfs, df], ignore_index=True)    \n",
    "    #dfs = pd.concat([dfs, df], ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874881f9-d60d-43aa-825f-afaefe6c58c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dfs[\"is_fail\"] == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "941385ac-da0e-47fd-800e-776f3ab7d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(d2l.HyperParameters):\n",
    "    \"\"\"The base class of data.\n",
    "\n",
    "    Defined in :numref:`subsec_oo-design-models`\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def get_dataloader(self, is_train=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_tensorloader([self.train_X, self.train_Y], is_train=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_tensorloader([self.val_X, self.val_Y], is_train=False)\n",
    "        \n",
    "    def thresh_dataloader(self):\n",
    "        return self.get_tensorloader([self.thresh_X, self.thresh_Y, self.thresh_labels], is_train=False)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return self.get_tensorloader([self.test_X, self.test_Y, self.test_labels], is_train=False)\n",
    "\n",
    "\n",
    "    def get_tensorloader(self, tensors, is_train, indices=slice(0, None)):\n",
    "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=is_train)\n",
    "\n",
    "class PlaneData(DataModule):\n",
    "    def __init__(self, data_frame, trunc_num = 5000, batch_size=64, \n",
    "                 num_steps = 50, train_val_ratio = 0.8, thresh_test_ratio = 0.5, is_test = False):\n",
    "        \"\"\"\n",
    "        So the normal flights (without anomalies. label = 0) will be the training dataset\n",
    "        Each normal flight is splitted into regression training and regression validation using `train__val_ratio`\n",
    "        Then, we will use anomalous flights to determine a validation threshold, using the same spltiting with `thresh_test_ratio` instead\n",
    "        So now the bad flights is splitted into threshold determination set and testing set \n",
    "        Finally, we use the testing dataset to compute all the metrics for a threshold\n",
    "        The second dataset (thresholding and test dataset) will contain a labels tensor\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # only visualizing `trunc_num` data points/rows.\n",
    "        # group by label (bad/normal flights), then group by flight_id, then sort data by timestamp\n",
    "        all_data = data_frame.iloc[:trunc_num]\n",
    "\n",
    "        start = time.time()\n",
    "        flight_groups = all_data.groupby(\"flight_id\") \n",
    "        flight_ids = list(flight_groups.groups.keys())\n",
    "\n",
    "        normal_flight_ids = [fid for fid in flight_ids if flight_groups.get_group(fid)[\"label\"].iloc[0] == 0]\n",
    "        bad_flight_ids = [fid for fid in flight_ids if flight_groups.get_group(fid)[\"label\"].iloc[0] == 1]\n",
    "        num_thresh_flights = int(len(bad_flight_ids) * thresh_test_ratio)\n",
    "        thresh_flight_ids = bad_flight_ids[:num_thresh_flights]\n",
    "        test_flight_ids = bad_flight_ids[num_thresh_flights:]\n",
    "\n",
    "        self.train_X = []\n",
    "        self.val_X = []\n",
    "        self.train_Y = []\n",
    "        self.val_Y = []\n",
    "        self.thresh_X, self.thresh_Y, self.thresh_labels = [], [], []\n",
    "        self.test_X, self.test_Y, self.test_labels = [], [], []\n",
    "        self.num_train = 0\n",
    "        self.num_val = 0\n",
    "        self.num_thresh = 0\n",
    "        self.num_test = 0\n",
    "\n",
    "        # sort each flight by time, then truncate to multiple of num_steps, then scale it separately from other flights\n",
    "        # Then combine it into the training/val sets\n",
    "        for fid in normal_flight_ids:\n",
    "            if 1 in flight[\"is_fail\"].values:\n",
    "                raise \"NUH UH\"\n",
    "                \n",
    "            flight = flight_groups.get_group(fid).sort_values(by=\"time\", kind=\"stable\")\n",
    "            flight = flight.drop([\"flight_id\", \"is_fail\", \"time\"], axis=1)\n",
    "            flight = flight.iloc[:len(flight) - (len(flight) % self.num_steps)]\n",
    "            \n",
    "            self.label_names = flight.columns.tolist()\n",
    "            #scaler = StandardScaler()\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(flight)\n",
    "\n",
    "            scaled = scaler.transform(flight) # scale \n",
    "            scaled_tensor = torch.tensor(scaled, dtype=torch.float32)\n",
    "\n",
    "            # Create input-output pairs\n",
    "            X_seqs, Y_seqs = self.create_sequences(scaled_tensor)\n",
    "            for i in range(1, X_seqs.shape[0]):\n",
    "                assert(X_seqs[i][-1].equal(Y_seqs[i-1]))\n",
    "\n",
    "            # split this flight into train/val sets\n",
    "            num_train = int(len(X_seqs) * self.train_val_ratio)\n",
    "            num_val = len(X_seqs) - num_train\n",
    "            self.num_train += num_train\n",
    "            self.num_val += num_val\n",
    "            \n",
    "            self.train_X.append(X_seqs[:num_train])\n",
    "            self.train_Y.append(Y_seqs[:num_train])\n",
    "            self.val_X.append(X_seqs[num_train:])\n",
    "            self.val_Y.append(Y_seqs[num_train:])\n",
    "\n",
    "        # After processing all flights, concatenate along the batch dimension\n",
    "        self.train_X = torch.cat(self.train_X, dim=0)  # (total_sequences, num_steps - 1, num_features)\n",
    "        self.train_Y = torch.cat(self.train_Y, dim=0)  # (total_sequences, num_features)\n",
    "        self.val_X = torch.cat(self.val_X, dim=0)  # (total_sequences, num_steps - 1, num_features)\n",
    "        self.val_Y = torch.cat(self.val_Y, dim=0)  # (total_sequences, num_features)\n",
    "        \n",
    "        # shape of X: (number of sequences, num_steps, # of features of the raw data) \n",
    "        # last dim is the number of different features (e.g. pitch, roll, etc) that each data point has\n",
    "        # X is input, Y is label.\n",
    "        # Y is the data point after X.\n",
    "        print(\"train_X's shape: \", self.train_X.shape)\n",
    "        print(\"train_Y's shape: \", self.train_Y.shape)\n",
    "        print(\"val_X's shape: \", self.val_X.shape)\n",
    "        print(\"val_Y's shape: \", self.val_Y.shape)\n",
    "\n",
    "        # Same thing as the above loop, with `thresh_flight_ids` instead\n",
    "        for fid in thresh_flight_ids:\n",
    "            flight = flight_groups.get_group(fid).sort_values(by=\"time\", kind=\"stable\")\n",
    "            flight = flight.iloc[:len(flight) - (len(flight) % self.num_steps)]\n",
    "            labels = flight[\"is_fail\"].values # this line is different from the first loop\n",
    "            flight = flight.drop([\"flight_id\", \"is_fail\", \"time\"], axis=1)\n",
    "\n",
    "            self.label_names = flight.columns.tolist()\n",
    "            #scaler = StandardScaler()\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(flight)\n",
    "\n",
    "            scaled = scaler.transform(flight) # scale \n",
    "            scaled_tensor = torch.tensor(scaled, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            # Create input-output pairs\n",
    "            X_seqs, Y_seqs, labels = self.create_sequences(scaled_tensor, labels_tensor)\n",
    "            for i in range(1, X_seqs.shape[0]):\n",
    "                assert(X_seqs[i][-1].equal(Y_seqs[i-1]))\n",
    "            \n",
    "            self.thresh_X.append(X_seqs)\n",
    "            self.thresh_Y.append(Y_seqs)\n",
    "            self.thresh_labels.append(labels)\n",
    "\n",
    "         # Same thing as the above loop, with `test_flight_ids` instead\n",
    "        for fid in thresh_flight_ids:\n",
    "            flight = flight_groups.get_group(fid).sort_values(by=\"time\", kind=\"stable\")\n",
    "            flight = flight.iloc[:len(flight) - (len(flight) % self.num_steps)]\n",
    "            labels = flight[\"is_fail\"].values # this line is different from the first loop\n",
    "            flight = flight.drop([\"flight_id\", \"is_fail\", \"time\"], axis=1)\n",
    "\n",
    "            self.label_names = flight.columns.tolist()\n",
    "            #scaler = StandardScaler()\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(flight)\n",
    "\n",
    "            scaled = scaler.transform(flight) # scale \n",
    "            scaled_tensor = torch.tensor(scaled, dtype=torch.float32)\n",
    "            labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            # Create input-output pairs\n",
    "            X_seqs, Y_seqs, labels = self.create_sequences(scaled_tensor, labels_tensor)\n",
    "            for i in range(1, X_seqs.shape[0]):\n",
    "                assert(X_seqs[i][-1].equal(Y_seqs[i-1]))\n",
    "            \n",
    "            self.test_X.append(X_seqs)\n",
    "            self.test_Y.append(Y_seqs)\n",
    "            self.test_labels.append(labels)\n",
    "            \n",
    "        # After processing all flights, concatenate along the batch dimension\n",
    "        self.thresh_X = torch.cat(self.thresh_X, dim=0)  # (total_sequences, num_steps - 1, num_features)\n",
    "        self.thresh_Y = torch.cat(self.thresh_Y, dim=0)  # (total_sequences, num_features)\n",
    "        self.thresh_labels = torch.cat(self.thresh_labels, dim=0) # (total_sequences, 1)\n",
    "        self.test_X = torch.cat(self.test_X, dim=0)  # (total_sequences, num_steps - 1, num_features)\n",
    "        self.test_Y = torch.cat(self.test_Y, dim=0)  # (total_sequences, num_features)\n",
    "        self.test_labels = torch.cat(self.test_labels, dim=0) # (total_sequences, 1)\n",
    "\n",
    "        # shape of X: (number of sequences, num_steps, # of features of the raw data) \n",
    "        # last dim is the number of different features (e.g. pitch, roll, etc) that each data point has\n",
    "        # X is input, Y is label.\n",
    "        # Y is the data point after X.\n",
    "        print(\"thresh_X's shape: \", self.thresh_X.shape)\n",
    "        print(\"thresh_Y's shape: \", self.thresh_Y.shape)\n",
    "        print(\"test_X's shape: \", self.test_X.shape)\n",
    "        print(\"test_Y's shape: \", self.test_Y.shape)\n",
    "        \n",
    "        end = time.time()\n",
    "        print(f\"Processing Time: {end - start}\")\n",
    "\n",
    "    def create_sequences(self, flight: torch.Tensor, labels: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Create input-output sequences from a single flight tensor.\n",
    "        \n",
    "        Args:\n",
    "            flight (torch.Tensor): Tensor of shape (num data points, num_features)\n",
    "        \n",
    "        Returns:\n",
    "            X (torch.Tensor): Input sequences of shape (num_sequences, num_steps - 1, num_features)\n",
    "            Y (torch.Tensor): Targets of shape (num_sequences, num_features)\n",
    "        \"\"\"\n",
    "        X, Y = [], []\n",
    "        num_seqs = flight.shape[0] - self.num_steps\n",
    "        if num_seqs <= 0:\n",
    "            raise f\"Not Enough Sequences. flight.shape[0] = {flight.shape[0]}, self.num_steps = {self.num_steps}\"\n",
    "        # broadcast then add. Shape of X_indices: (num_seqs, num_steps)\n",
    "        # basically, X_indices[i] = [i, i + 1, ... , i + self.num_steps - 1] is the i-th sequence from the dataset\n",
    "        X_indices = torch.arange(num_seqs)[:, None] + torch.arange(self.num_steps) \n",
    "        X = flight[X_indices] # Shape of X: (num_seqs, num_steps, num_features)\n",
    "        Y = flight[self.num_steps:] # Shape of Y: (num_seqs, num_features)\n",
    "        if labels is None:\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X, Y, labels[self.num_steps:]\n",
    "        \n",
    "\n",
    "        # broadcast then add. Shape of X_indices: (num_seqs, num_steps)\n",
    "        \n",
    "    # def get_dataloader(self, X, Y, is_train=False):\n",
    "    #     if is_train:\n",
    "    #         return self.get_tensorloader([self.train_X, self.train_Y], is_train, slice(0, None))\n",
    "    #     else:\n",
    "    #         return self.get_tensorloader([self.val_X, self.val_Y], is_train, slice(0, None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a5a2d682-c09e-4a41-a3e8-35c1a7d49877",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P, shape of P: (max sequence length, input embeddings dim = num_hiddens)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        # initialize the positional encoding\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        # 0::2 in the third dimension means \"select every second element starting from index 0.\"\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # make sure the second dim matches\n",
    "        X = X + self.P[:, X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(num_hiddens)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dense2 = nn.LazyLinear(num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.gelu(self.dense1(X)))\n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_hiddens_ffn, num_blks, num_hiddens_latent, \n",
    "                 num_heads=4, dropout=0.2, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "        # self.embed = nn.LazyLinear(num_hiddens) # project to higher dimension\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(self.num_blks):\n",
    "            self.blks.add_module(f\"blk#{i}\", TransformerEncoder(num_hiddens, num_hiddens_ffn, num_heads, \n",
    "                                                                dropout, bias))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.pos_encoding(self.embed(X))\n",
    "        \n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_hiddens_ffn, num_blks, num_heads, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "692ae658-25aa-4ba4-bf88-9aa366a7dad8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "class RNN(d2l.Module):\n",
    "    def __init__(self, num_features, num_hiddens, num_hiddens_ffn, num_hiddens_latent, \n",
    "                 num_lstm_layers, dropout=0.2, bias=True, lr = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.LSTM(num_features, num_hiddens, num_layers = 2, batch_first = True)\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, X, H_C = None):\n",
    "        output, H_C = self.rnn(X, H_C)\n",
    "        return output, H_C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cf53bb0d-059f-4672-91c0-fcfe64889a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(d2l.Module):\n",
    "    def __init__(self, rnn, num_features, threshold = 0.1, lr = 0.1, wd = 1e-5):\n",
    "        \"\"\"\n",
    "        num_features: how many features are there? Is the last dim of a batch: (batch_size, num_steps, num_inputs) \n",
    "        num_hiddens: dim for each variable (e.g. torque, yaw, etc)\n",
    "        num_hiddens_ffn: for transformer\n",
    "        num_blks: # enc blocks for transformer \n",
    "        num_heads: # heads for transformer encoder block\n",
    "        num_hiddens_latent: for AE\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = rnn\n",
    "        self.rnn.init_weights()\n",
    "        #self.dense1 = nn.LazyLinear(int(rnn.num_hiddens / 2))\n",
    "        self.dense2 = nn.LazyLinear(num_features)\n",
    "        self.init_weights()\n",
    "        self.train_loss, self.val_loss = [], []\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.apply(init_normal)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        `output`: final output of the each cell in the last layer, depth-wise\n",
    "        `H`: final output of the last cell in a sequence in the batch, timestep-wise\n",
    "        Check my notebook in Notion for a visualization of this\n",
    "        output's shape: [batch size, num steps, hidden size)\n",
    "        H's output: [batch size, hidden size]\n",
    "        both output[:, -1, :] and h_n[-1, :, :] give (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (H, _) = self.rnn(X)\n",
    "        output = self.dense2(H[-1])\n",
    "        #output = self.dense2(self.dense1(H[-1])) # we have 2 LSTM layers, take the last one\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        # right now Y_hat is the output of the dense after the RNN with dim (batch_size, 1, num_features)\n",
    "        fn = nn.MSELoss()\n",
    "        #print(Y_hat.shape, Y.shape)\n",
    "        return fn(Y_hat, Y)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        Y = batch[-1]\n",
    "        l = self.loss(Y_hat, Y)        \n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "        \n",
    "    def validation_step(self, batch):\n",
    "        Y_hat = self(*batch[:-1])\n",
    "        Y = batch[-1]\n",
    "        l = self.loss(Y_hat, Y) \n",
    "        self.plot('loss', l, train=False)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "685d0569-6dcc-4d5a-b74b-fd63ddd398c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.Trainer):\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        super().__init__(max_epochs)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.thresh_dataloader = data.thresh_dataloader()\n",
    "        self.test_dataloader = data.test_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "\n",
    "    def fit(self, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optim, milestones=[5, 15, 30],gamma = 0.1)\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "            self.scheduler.step()\n",
    "            plt.savefig(\"./losses.png\")\n",
    "            \n",
    "        self.find_best_threshold()\n",
    "        self.evaluate_thresholds()      \n",
    "\n",
    "    def fit_epoch(self):\n",
    "        \"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(self.prepare_batch(batch))\n",
    "            self.optim.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                if self.gradient_clip_val > 0:  # To be discussed later\n",
    "                    self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "                self.optim.step()\n",
    "                \n",
    "            self.train_batch_idx += 1\n",
    "            self.train_losses.append(float(loss))\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "        self.model.eval()\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                loss = self.model.validation_step(self.prepare_batch(batch))\n",
    "                self.val_losses.append(float(loss))\n",
    "            self.val_batch_idx += 1\n",
    "\n",
    "    def find_best_threshold(self):\n",
    "        self.model.eval()\n",
    "        \n",
    "        max_errors_list, labels_list = [], []\n",
    "        for batch in self.thresh_dataloader:\n",
    "            X, Y, labels = self.prepare_batch(batch)\n",
    "            Y_hat = self.model(X)\n",
    "            # shape of errors: (batch_size, num_features). \n",
    "            # Didn't use the self.model.loss function here as it will use a mean reduction over all features. \n",
    "            errors = (Y_hat - Y) ** 2\n",
    "            max_errors = errors.max(dim=1)[0] # for each batch, get the error of the feature that produces the greatest error\n",
    "            max_errors_list.append(max_errors.detach().cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "\n",
    "        thresh_max_errors = np.concatenate(max_errors_list)\n",
    "        thresh_labels = np.concatenate(labels_list)\n",
    "        precision, recall, self.thresholds = precision_recall_curve(thresh_labels, thresh_max_errors, pos_label=1)\n",
    "        f1_scores = 2*(precision*recall) / (precision + recall + 1e-9)\n",
    "        best_threshold_idx = np.argmax(f1_scores)\n",
    "        best_threshold = self.thresholds[best_threshold_idx]\n",
    "        best_f1 = f1_scores[best_threshold_idx]\n",
    "\n",
    "        self.best_val_threshold_idx = best_threshold_idx\n",
    "        self.best_threshold = best_threshold\n",
    "        self.best_val_f1 = best_f1\n",
    "\n",
    "        anomalies = thresh_max_errors > best_threshold\n",
    "        tp = np.sum(anomalies & (thresh_labels == 1))\n",
    "        fp = np.sum(anomalies & (thresh_labels == 0))\n",
    "        tn = np.sum(~anomalies & (thresh_labels == 0))\n",
    "        fn = np.sum(~anomalies & (thresh_labels == 1))\n",
    "        prec = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        rec = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0\n",
    "        \n",
    "        print(f\"Optimal Threshold (Thresholding Set): {best_threshold:.6f}\")\n",
    "        print(f\"Thresholding Set Metrics: Precision: {prec:.6f}, Recall: {rec:.6f}, F1: {best_f1:.6f}, Accuracy: {acc:.6f}\")\n",
    "\n",
    "    def evaluate_thresholds(self):\n",
    "        \"\"\"\n",
    "        Find reconstruction errors for the test set. Then for each threshold in self.thresholds,\n",
    "        compute all the metrics against that threshold and graph them\n",
    "        \"\"\"\n",
    "        if self.best_val_f1 is None:\n",
    "            raise \"Please run find_best_threshold() first!\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        max_errors_list, labels_list = [], []\n",
    "        for batch in self.test_dataloader:\n",
    "            X, Y, labels = self.prepare_batch(batch)\n",
    "            Y_hat = self.model(X)\n",
    "            # shape of errors: (batch_size, num_features). \n",
    "            # Didn't use the self.model.loss function here as it will use a mean reduction over all features. \n",
    "            errors = (Y_hat - Y) ** 2\n",
    "            max_errors = errors.max(dim=1)[0] # for each batch, get the error of the feature that produces the greatest error\n",
    "            max_errors_list.append(max_errors.detach().cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "            \n",
    "        test_max_errors = np.concatenate(max_errors_list)\n",
    "        test_labels = np.concatenate(labels_list)\n",
    "        test_precision = []\n",
    "        test_recall = []\n",
    "        test_f1 = []\n",
    "        test_accuracy = []\n",
    "        for thresh in self.thresholds:\n",
    "            anomalies = test_max_errors > thresh\n",
    "            tp = np.sum(anomalies & (test_labels == 1))\n",
    "            fp = np.sum(anomalies & (test_labels == 0))\n",
    "            tn = np.sum(~anomalies & (test_labels == 0))\n",
    "            fn = np.sum(~anomalies & (test_labels == 1))\n",
    "            prec = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "            rec = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "            f1 = 2 * (prec * rec) / (prec + rec + 1e-9) if prec + rec > 0 else 0\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0\n",
    "            test_precision.append(prec)\n",
    "            test_recall.append(rec)\n",
    "            test_f1.append(f1)\n",
    "            test_accuracy.append(acc)\n",
    "            \n",
    "        best_test_idx = np.argmax(test_f1)\n",
    "        best_test_threshold = self.thresholds[best_test_idx]\n",
    "        print(f\"Best Test Set Threshold: {best_test_threshold:.6f}\")\n",
    "        print(f\"Test Set Metrics at Best Threshold: Precision: {test_precision[best_test_idx]:.6f}, \"\n",
    "              f\"Recall: {test_recall[best_test_idx]:.6f}, F1: {test_f1[best_test_idx]:.6f}, \"\n",
    "              f\"Accuracy: {test_accuracy[best_test_idx]:.6f}\")\n",
    "        \n",
    "        # Visualize metrics vs. thresholds\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.thresholds, test_precision, label=\"Precision\", marker='.')\n",
    "        plt.plot(self.thresholds, test_recall, label=\"Recall\", marker='.')\n",
    "        plt.plot(self.thresholds, test_f1, label=\"F1 Score\", marker='.')\n",
    "        plt.plot(self.thresholds, test_accuracy, label=\"Accuracy\", marker='.')\n",
    "        plt.axvline(x=self.best_threshold, color='red', linestyle='--', label=f\"Optimal Threshold ({self.best_threshold:.6f})\")\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.ylabel(\"Metric Value\")\n",
    "        plt.title(\"Metrics vs. Threshold on Test Set\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"./metrics/metrics_vs_threshold.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        test_precision, test_recall, _ = precision_recall_curve(test_labels, test_max_errors, pos_label=1)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(test_recall, test_precision, label=\"Precision-Recall Curve\")\n",
    "        plt.scatter(test_recall[best_test_idx], test_precision[best_test_idx], color='red',\n",
    "                    label=f\"Best Threshold ({best_test_threshold:.6f})\")\n",
    "        plt.xlabel(\"Recall\")\n",
    "        plt.ylabel(\"Precision\")\n",
    "        plt.title(\"Precision-Recall Curve (Test Set)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"./metrics/precision_recall_curve_test.png\")\n",
    "        plt.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "74c3c5f0-c7ae-4cda-8307-2bcc6c70380d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X's shape:  torch.Size([236000, 50, 10])\n",
      "train_Y's shape:  torch.Size([236000, 10])\n",
      "val_X's shape:  torch.Size([59000, 50, 10])\n",
      "val_Y's shape:  torch.Size([59000, 10])\n",
      "thresh_X's shape:  torch.Size([73750, 50, 10])\n",
      "thresh_Y's shape:  torch.Size([73750, 10])\n",
      "test_X's shape:  torch.Size([73750, 50, 10])\n",
      "test_Y's shape:  torch.Size([73750, 10])\n",
      "Processing Time: 2.0305120944976807\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50\n",
    "data = PlaneData(dfs, trunc_num=-1, num_steps=num_steps, is_test=False)\n",
    "#print(data.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734df40-e986-4351-9049-29e484cf8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data.train_X.shape[2] \n",
    "num_hiddens, num_hiddens_ffn, num_blks, num_hiddens_latent = 128, 128, 4, 64\n",
    "num_lstm_layers = 2\n",
    "num_heads, dropout, bias, lr = 4, 0.2, True, 1e-2\n",
    "rnn = RNN(num_features, num_hiddens, num_hiddens_ffn, num_hiddens_latent, \n",
    "             num_lstm_layers, dropout, bias, lr)\n",
    "model = MainModel(rnn, num_features, lr)\n",
    "\n",
    "trainer = Trainer(max_epochs=70, num_gpus=2)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c86d44-7bde-4bd7-96c9-a15877eaa065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb9938-6ae7-4a81-8381-9c17c5465dc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# col_names = data.label_names\n",
    "\n",
    "# # get 5 sequences, then flatten the first 2 dim out to be the pseudo-time dim so we can graph it.\n",
    "# data_X = data.val_X[:8]\n",
    "# data_Y = data.val_Y[:8]\n",
    "# num_graphs = data_Y.shape[1]\n",
    "# # t = torch.arange(data_X.shape[0]*data_X.shape[1])\n",
    "\n",
    "# print(data_X.shape, data_Y.shape)\n",
    "# for i in range(num_graphs):\n",
    "#     fig, axes = plt.subplots(1, figsize=(13, 4), sharex=\"col\", sharey=\"row\")\n",
    "#     plt.subplots_adjust(hspace=0.05, wspace=0.05)  # Reduced wspace from default (~0.2) to 0.1\n",
    "\n",
    "#     # .reshape(-1, data.X.shape[2])\n",
    "#     # for each label, plot it with its input\n",
    "#     axes.set_ylabel(col_names[i])\n",
    "#     axes.legend([\"Input\", \"Label\"])\n",
    "#     num_steps = data_X.shape[1]\n",
    "#     t_Y = -1\n",
    "#     for j in range(data_Y.shape[0]):\n",
    "#         t_X = torch.arange(start = t_Y + 1, end = t_Y + num_steps + 1) \n",
    "#         t_Y = t_Y + num_steps + 1\n",
    "#         axes.plot(t_X, data_X[j, :, i]) # input sequence\n",
    "#         axes.plot(t_Y, data_Y[j, i], color=\"red\", marker='o', markersize=10) # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7eb80a3-72bd-4518-95a4-d8f7d41cac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./losses.txt\", \"w\") as f:\n",
    "    f.write(\", \".join([str(i) for i in trainer.train_losses]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\", \".join([str(i) for i in trainer.val_losses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2bfc35-5daa-4c97-b440-de97d5871d0c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mLazyLinear(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m      3\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mMultiStepLR(optimizer, milestones\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m45\u001b[39m], gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "net = nn.LazyLinear(5) # dummy net\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 7, 9, 15, 20, 25, 40, 45], gamma = 0.5)\n",
    "\n",
    "def get_lr(optimizer, scheduler):\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(lr)\n",
    "    return lr\n",
    "\n",
    "d2l.plot(torch.arange(100), [get_lr(optimizer, scheduler)\n",
    "                                  for t in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf1efaa7-cf5c-4c82-ad70-bfa77b2fb765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10,  6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1,2,3, 4],\n",
    "                  [7,1,9, 10],\n",
    "                  [4,5,6, 2]])\n",
    "t.max(dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ed90e92-545d-4fd0-92be-9e50caa86f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, t = precision_recall_curve([0, 1, 0, 1, 0, 0, 0, 1], [0.02, 0.08, 0.02, 0.09, 0.04, 0.04, 0.03, 0.07])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c11069d2-2639-402b-83ea-bfc3dddb8cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.375, 0.5  , 0.6  , 1.   , 1.   , 1.   , 1.   ]),\n",
       " array([1.        , 1.        , 1.        , 1.        , 0.66666667,\n",
       "        0.33333333, 0.        ]),\n",
       " array([0.02, 0.03, 0.04, 0.07, 0.08, 0.09]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p, r, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2c04831b-3ccf-4b6b-a0fd-693f1d691016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70169ae-4ee8-42fc-95b7-7153aa294707",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
