{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3cd77cb-31c3-4072-8a83-1f5de04401ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7bf8400c29f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import d2l.torch as d2l\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f6143-1f52-4f96-8455-a3f44c791aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.DataFrame()\n",
    "for i in range(1, 6):\n",
    "    df = pd.read_excel(f\"./data/flight_data_batch{i}.xlsx\")\n",
    "    dfs = pd.concat([dfs, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d73bdf-9560-45d2-8e9c-6d1cf23ab20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 3):\n",
    "    df = pd.read_excel(f\"./data/flight_data_batch6_part{i}.xlsx\")\n",
    "    dfs = pd.concat([dfs, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "941385ac-da0e-47fd-800e-776f3ab7d7eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DataModule(d2l.HyperParameters):\n",
    "    \"\"\"The base class of data.\n",
    "\n",
    "    Defined in :numref:`subsec_oo-design-models`\"\"\"\n",
    "    def __init__(self, root='../data', num_workers=4):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def get_dataloader(self, is_train=True):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.get_dataloader(is_train=True)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.get_dataloader(is_train=False)\n",
    "\n",
    "\n",
    "    def get_tensorloader(self, tensors, is_train, indices=slice(0, None)):\n",
    "        \"\"\"Defined in :numref:`sec_synthetic-regression-data`\"\"\"\n",
    "        tensors = tuple(a[indices] for a in tensors)\n",
    "        dataset = torch.utils.data.TensorDataset(*tensors)\n",
    "        return torch.utils.data.DataLoader(dataset, self.batch_size, shuffle=is_train)\n",
    "\n",
    "class PlaneData(DataModule):\n",
    "    def __init__(self, data_frame, trunc_num = 5000, batch_size=64, \n",
    "                 num_steps = 50, train_val_ratio = 0.8, is_test = False, num_cols=None):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # only visualizing `trunc_num` data points/rows.\n",
    "        # group by label (bad/normal flights), then group by flight_id, then sort data by timestamp\n",
    "        all_data = data_frame.iloc[:trunc_num]\n",
    "\n",
    "        start = time.time()\n",
    "        all_data = all_data.groupby(\"label\") \n",
    "\n",
    "        normal_groups = all_data.get_group(0)\n",
    "\n",
    "        normal_flights = normal_groups.groupby(\"flight_id\")\n",
    "        normal_flights = [f for _, f in normal_flights] # a list now\n",
    "    \n",
    "\n",
    "        self.train_X = []\n",
    "        self.val_X = []\n",
    "        self.train_Y = []\n",
    "        self.val_Y = []\n",
    "        self.num_train = 0\n",
    "        self.num_val = 0\n",
    "\n",
    "        # sort each flight by time, then truncate to multiple of num_steps, then scale it separately from other flights\n",
    "        # Then combine it into the training/val sets\n",
    "        for flight in normal_flights:\n",
    "            flight = flight.sort_values(by=\"time\", kind=\"stable\")\n",
    "            flight = flight.drop([\"time\", \"flight_id\"], axis=1)\n",
    "            flight = flight.iloc[:len(flight) - (len(flight) % self.num_steps)]\n",
    "            \n",
    "            if num_cols:\n",
    "                flight = flight.iloc[:, :num_cols]\n",
    "            \n",
    "            self.label_names = flight.columns.tolist()\n",
    "            #scaler = StandardScaler()\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(flight)\n",
    "\n",
    "            scaled = scaler.transform(flight) # scale \n",
    "            scaled_tensor = torch.tensor(scaled, dtype=torch.float32)\n",
    "\n",
    "            # Create input-output pairs\n",
    "            X_seqs, Y_seqs = self.create_sequences(scaled_tensor)\n",
    "            for i in range(1, X_seqs.shape[0]):\n",
    "                assert(X_seqs[i][-1].equal(Y_seqs[i-1]))\n",
    "\n",
    "            # split this flight into train/val sets\n",
    "            num_train = int(len(X_seqs) * self.train_val_ratio)\n",
    "            num_val = len(X_seqs) - num_train\n",
    "            self.num_train += num_train\n",
    "            self.num_val += num_val\n",
    "            \n",
    "            self.train_X.append(X_seqs[:num_train])\n",
    "            self.train_Y.append(Y_seqs[:num_train])\n",
    "            self.val_X.append(X_seqs[num_train:])\n",
    "            self.val_Y.append(Y_seqs[num_train:])\n",
    "\n",
    "        # After processing all flights, concatenate along the batch dimension\n",
    "        self.train_X = torch.cat(self.train_X, dim=0)  # (total_sequences, num_steps - 1, num_features)\n",
    "        self.train_Y = torch.cat(self.train_Y, dim=0)  # (total_sequences, num_features)\n",
    "        self.val_X = torch.cat(self.val_X, dim=0)  # (total_sequences, num_steps - 1, num_features)\n",
    "        self.val_Y = torch.cat(self.val_Y, dim=0)  # (total_sequences, num_features)\n",
    "        \n",
    "        # shape of X: (number of sequences, num_steps, # of features of the raw data) \n",
    "        # last dim is the number of different features (e.g. pitch, roll, etc) that each data point has\n",
    "        # X is input, Y is label.\n",
    "        # Y is the data point after X.\n",
    "        print(\"train_X's shape: \", self.train_X.shape)\n",
    "        print(\"train_Y's shape: \", self.train_Y.shape)\n",
    "        print(\"val_X's shape: \", self.val_X.shape)\n",
    "        print(\"val_Y's shape: \", self.val_Y.shape)\n",
    "        end = time.time()\n",
    "        print(f\"Processing Time: {end - start}\")\n",
    "\n",
    "        \n",
    "    # def create_sequences(self, flight: torch.Tensor):\n",
    "    #     \"\"\"\n",
    "    #     Create input-output sequences from a single flight tensor.\n",
    "        \n",
    "    #     Args:\n",
    "    #         flight (torch.Tensor): Tensor of shape (num data points, num_features)\n",
    "        \n",
    "    #     Returns:\n",
    "    #         X (torch.Tensor): Input sequences of shape (num_sequences, num_steps - 1, num_features)\n",
    "    #         Y (torch.Tensor): Targets of shape (num_sequences, num_features)\n",
    "    #     \"\"\"\n",
    "    #     X, Y = [], []\n",
    "    \n",
    "    #     for i in range(flight.shape[0] - self.num_steps):\n",
    "    #         seq = flight[i:i + self.num_steps]\n",
    "    #         X.append(seq[:-1])  # first num_steps - 1 as input\n",
    "    #         Y.append(seq[-1])   # last step as prediction target\n",
    "    \n",
    "    #     X = torch.stack(X)  # (num_sequences, num_steps - 1, num_features)\n",
    "    #     Y = torch.stack(Y)  # (num_sequences, num_features)\n",
    "    \n",
    "    #     #print(f\"create_sequences: X.shape = {X.shape}, Y.shape = {Y.shape}\")\n",
    "    #     return X, Y\n",
    "    def create_sequences(self, flight: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Create input-output sequences from a single flight tensor.\n",
    "        \n",
    "        Args:\n",
    "            flight (torch.Tensor): Tensor of shape (num data points, num_features)\n",
    "        \n",
    "        Returns:\n",
    "            X (torch.Tensor): Input sequences of shape (num_sequences, num_steps - 1, num_features)\n",
    "            Y (torch.Tensor): Targets of shape (num_sequences, num_features)\n",
    "        \"\"\"\n",
    "        X, Y = [], []\n",
    "        num_seqs = flight.shape[0] - self.num_steps\n",
    "        if num_seqs <= 0:\n",
    "            raise f\"Not Enough Sequences. flight.shape[0] = {flight.shape[0]}, self.num_steps = {self.num_steps}\"\n",
    "        # broadcast then add. Shape of X_indices: (num_seqs, num_steps)\n",
    "        # basically, X_indices[i] = [i, i + 1, ... , i + self.num_steps - 1] is the i-th sequence from the dataset\n",
    "        X_indices = torch.arange(num_seqs)[:, None] + torch.arange(self.num_steps) \n",
    "        X = flight[X_indices] # Shape of X: (num_seqs, num_steps, num_features)\n",
    "        Y = flight[self.num_steps:] # Shape of Y: (num_seqs, num_features)\n",
    "        return X, Y\n",
    "        \n",
    "\n",
    "        # broadcast then add. Shape of X_indices: (num_seqs, num_steps)\n",
    "        \n",
    "    def get_dataloader(self, is_train=False):\n",
    "        if self.is_test:\n",
    "            # overfit 1 single batch to verify that we can reach the lowest training loss (0)\n",
    "            X, Y = self.X[1240:1240+self.batch_size], self.Y[1240:1240+self.batch_size] # get a batch\n",
    "            X = X.repeat(20, 1, 1)  # replicate this batch\n",
    "            Y = Y.repeat(20, 1) # replicate this batch\n",
    "            #print(X.shape, Y.shape)\n",
    "            idx = slice(0, len(X)-self.batch_size) if is_train else slice(len(X)-self.batch_size, None) \n",
    "            return self.get_tensorloader([X, Y], is_train, idx)\n",
    "        else:\n",
    "            if is_train:\n",
    "                return self.get_tensorloader([self.train_X, self.train_Y], is_train, slice(0, None))\n",
    "            else:\n",
    "                return self.get_tensorloader([self.val_X, self.val_Y], is_train, slice(0, None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5a2d682-c09e-4a41-a3e8-35c1a7d49877",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_hiddens, dropout, max_len=1000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P, shape of P: (max sequence length, input embeddings dim = num_hiddens)\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        # initialize the positional encoding\n",
    "        X = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) / torch.pow(\n",
    "            10000, torch.arange(0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        # 0::2 in the third dimension means \"select every second element starting from index 0.\"\n",
    "        self.P[:, :, 0::2] = torch.sin(X)\n",
    "        self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # make sure the second dim matches\n",
    "        X = X + self.P[:, X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "class PositionwiseFFN(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_outputs):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(num_hiddens)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dense2 = nn.LazyLinear(num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense2(self.gelu(self.dense1(X)))\n",
    "        \n",
    "        \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_hiddens_ffn, num_blks, num_hiddens_latent, \n",
    "                 num_heads=4, dropout=0.2, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "        # self.embed = nn.LazyLinear(num_hiddens) # project to higher dimension\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(self.num_blks):\n",
    "            self.blks.add_module(f\"blk#{i}\", TransformerEncoder(num_hiddens, num_hiddens_ffn, num_heads, \n",
    "                                                                dropout, bias))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.pos_encoding(self.embed(X))\n",
    "        \n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_hiddens_ffn, num_blks, num_heads, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_blks = num_blks\n",
    "        \n",
    "    def forward(self, X):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692ae658-25aa-4ba4-bf88-9aa366a7dad8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def init_normal(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "\n",
    "class RNN(d2l.Module):\n",
    "    def __init__(self, num_features, num_hiddens, num_hiddens_ffn, num_hiddens_latent, \n",
    "                 num_lstm_layers, dropout=0.2, bias=True, lr = 0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.LSTM(num_features, num_hiddens, num_layers = 2, batch_first = True)\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.apply(init_normal)\n",
    "\n",
    "    def forward(self, X, H_C = None):\n",
    "        output, H_C = self.rnn(X, H_C)\n",
    "        return output, H_C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53bb0d-059f-4672-91c0-fcfe64889a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModel(d2l.Module):\n",
    "    def __init__(self, rnn, num_features, lr = 0.1, wd = 1e-5):\n",
    "        \"\"\"\n",
    "        num_features: how many features are there? Is the last dim of a batch: (batch_size, num_steps, num_inputs) \n",
    "        num_hiddens: dim for each variable (e.g. torque, yaw, etc)\n",
    "        num_hiddens_ffn: for transformer\n",
    "        num_blks: # enc blocks for transformer \n",
    "        num_heads: # heads for transformer encoder block\n",
    "        num_hiddens_latent: for AE\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = rnn\n",
    "        self.rnn.init_weights()\n",
    "        #self.dense1 = nn.LazyLinear(int(rnn.num_hiddens / 2))\n",
    "        self.dense2 = nn.LazyLinear(num_features)\n",
    "        self.init_weights()\n",
    "        self.train_loss, self.val_loss = [], []\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.apply(init_normal)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        `output`: final output of the each cell in the last layer, depth-wise\n",
    "        `H`: final output of the last cell in a sequence in the batch, timestep-wise\n",
    "        Check my notebook in Notion for a visualization of this\n",
    "        output's shape: [batch size, num steps, hidden size)\n",
    "        H's output: [batch size, hidden size]\n",
    "        both output[:, -1, :] and h_n[-1, :, :] give (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        _, (H, _) = self.rnn(X)\n",
    "        output = self.dense2(H[-1])\n",
    "        #output = self.dense2(self.dense1(H[-1])) # we have 2 LSTM layers, take the last one\n",
    "        #print(output.shape)\n",
    "        return output\n",
    "\n",
    "    def loss(self, Y_hat, Y):\n",
    "        # right now Y_hat is the output of the dense after the RNN with dim (batch_size, 1, num_features)\n",
    "        fn = nn.HuberLoss()\n",
    "        #print(Y_hat.shape, Y.shape)\n",
    "        return fn(Y_hat, Y)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])        \n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "        \n",
    "    def validation_step(self, batch):\n",
    "        #print(len(batch), batch[0].shape, batch[-1].shape)\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])        \n",
    "        self.plot('loss', l, train=False)\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d0569-6dcc-4d5a-b74b-fd63ddd398c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.Trainer):\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        super().__init__(max_epochs)\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.gpus = [d2l.gpu(i) for i in range(min(num_gpus, d2l.num_gpus()))]\n",
    "\n",
    "         \n",
    "    def fit(self, model, data):\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim, self.scheduler = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            self.fit_epoch()\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        \"\"\"Defined in :numref:`sec_linear_scratch`\"\"\"\n",
    "        self.model.train()\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(self.prepare_batch(batch))\n",
    "            self.optim.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                loss.backward()\n",
    "                if self.gradient_clip_val > 0:  # To be discussed later\n",
    "                    self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "                self.optim.step()\n",
    "                self.scheduler.step(loss)\n",
    "            self.train_batch_idx += 1\n",
    "            self.train_losses.append(float(loss))\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "        self.model.eval()\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                loss = self.model.validation_step(self.prepare_batch(batch))\n",
    "                self.val_losses.append(float(loss))\n",
    "            self.val_batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3c5f0-c7ae-4cda-8307-2bcc6c70380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "data = PlaneData(dfs, trunc_num=-1, num_steps=num_steps, is_test=False, num_cols = None)\n",
    "#print(data.Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734df40-e986-4351-9049-29e484cf8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data.train_X.shape[2] \n",
    "num_hiddens, num_hiddens_ffn, num_blks, num_hiddens_latent = 128, 128, 4, 64\n",
    "num_lstm_layers = 2\n",
    "num_heads, dropout, bias, lr = 4, 0.2, True, 1e-4\n",
    "rnn = RNN(num_features, num_hiddens, num_hiddens_ffn, num_hiddens_latent, \n",
    "             num_lstm_layers, dropout, bias, lr)\n",
    "model = MainModel(rnn, num_features, lr)\n",
    "\n",
    "trainer = Trainer(max_epochs=50, num_gpus=2)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c86d44-7bde-4bd7-96c9-a15877eaa065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb9938-6ae7-4a81-8381-9c17c5465dc9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# col_names = data.label_names\n",
    "\n",
    "# # get 5 sequences, then flatten the first 2 dim out to be the pseudo-time dim so we can graph it.\n",
    "# data_X = data.val_X[:8]\n",
    "# data_Y = data.val_Y[:8]\n",
    "# num_graphs = data_Y.shape[1]\n",
    "# # t = torch.arange(data_X.shape[0]*data_X.shape[1])\n",
    "\n",
    "# print(data_X.shape, data_Y.shape)\n",
    "# for i in range(num_graphs):\n",
    "#     fig, axes = plt.subplots(1, figsize=(13, 4), sharex=\"col\", sharey=\"row\")\n",
    "#     plt.subplots_adjust(hspace=0.05, wspace=0.05)  # Reduced wspace from default (~0.2) to 0.1\n",
    "\n",
    "#     # .reshape(-1, data.X.shape[2])\n",
    "#     # for each label, plot it with its input\n",
    "#     axes.set_ylabel(col_names[i])\n",
    "#     axes.legend([\"Input\", \"Label\"])\n",
    "#     num_steps = data_X.shape[1]\n",
    "#     t_Y = -1\n",
    "#     for j in range(data_Y.shape[0]):\n",
    "#         t_X = torch.arange(start = t_Y + 1, end = t_Y + num_steps + 1) \n",
    "#         t_Y = t_Y + num_steps + 1\n",
    "#         axes.plot(t_X, data_X[j, :, i]) # input sequence\n",
    "#         axes.plot(t_Y, data_Y[j, i], color=\"red\", marker='o', markersize=10) # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7eb80a3-72bd-4518-95a4-d8f7d41cac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./losses.txt\", \"w\") as f:\n",
    "    f.write(\", \".join([str(i) for i in trainer.train_losses]))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\", \".join([str(i) for i in trainer.val_losses]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
